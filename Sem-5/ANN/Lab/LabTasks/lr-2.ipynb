{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "# Print the tensor\n",
    "print(tensor_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  7, 15])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create two arrays using NumPy\n",
    "array_a = np.array([1, 2, 3])\n",
    "array_b = np.array([4, 5, 6])\n",
    "\n",
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a\n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a * tensor_b\n",
    "\n",
    "# Add tensor_c to tensor_d\n",
    "tensor_e = tensor_c + tensor_d\n",
    "\n",
    "print(tensor_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1552]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the input tensor\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 5),  # First layer\n",
    "    nn.Linear(5, 1),  # Second layer\n",
    "    nn.Sigmoid()      # Activation function (Sigmoid)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1896, 0.2423]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network with three hidden layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(12, 20),  \n",
    "    nn.Linear(20, 14),  \n",
    "    nn.Linear(14, 3),   \n",
    "    nn.Linear(3, 2)     \n",
    ")\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1, 5, 3, 6, 9]])\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise**\n",
    "Neural networks are a core component of deep learning models. They can power so much in your daily life, from language translation apps to the cameras on your smartphone. Which of the following statements about neural networks is true?\n",
    "\n",
    "Answer the question(ad)\n",
    "\n",
    "**Select all correct answers**\n",
    "\n",
    "\n",
    "1). A neural network with a single linear layer followed by a sigmoid activation is similar to a logistic regression model.\n",
    "\n",
    "\n",
    "2). A neural network can only contain two linear layers.\n",
    "\n",
    "\n",
    "3). The softmax function takes a tensor of dimension N as input and outputs a float between zero and one.\n",
    "\n",
    "\n",
    "4). The input dimension of a linear layer must be equal to the output dimension of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Answers:\n",
    "1) True\n",
    "4) True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Output: tensor([[0.6900]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[0.8]])\n",
    "\n",
    "sigmoid = nn.Sigmoid()  \n",
    "probability = sigmoid(input_tensor) \n",
    "print(\"Sigmoid Output:\", probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output: tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1) \n",
    "probability_softmax = softmax(input_tensor)  \n",
    "print(\"Softmax Output:\", probability_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Probability: tensor([[0.0952]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 1),  \n",
    "    nn.Sigmoid()      \n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Output Probability:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Output: tensor([[-1.1923]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),  \n",
    "    nn.Linear(20, 15),  \n",
    "    nn.Linear(15, 10),  \n",
    "    nn.Linear(10, 1)    \n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Regression Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Classification Output: tensor([[0.2135, 0.2657, 0.2482, 0.2725]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),  \n",
    "    nn.Linear(20, 15),  \n",
    "    nn.Linear(15, 10), \n",
    "    nn.Linear(10, 4),   \n",
    "    nn.Softmax(dim=1)  \n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(\"Multi-class Classification Output:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
