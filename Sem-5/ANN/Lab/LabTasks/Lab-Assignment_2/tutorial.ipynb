{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***List all necessary preprocessing techniques required to prepare the data for embedding into a neural network.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare data for embedding into a neural network, several preprocessing techniques are typically required to ensure that the data is clean, standardized, and properly formatted. Here’s a list of key preprocessing steps depending on the type of data (numerical, textual, categorical, etc.):\n",
    "\n",
    "### 1. **Handling Missing Data**\n",
    "   - **Remove missing values**: Rows or columns with too many missing values can be dropped.\n",
    "   - **Imputation**: Replace missing values using statistical methods (mean, median, mode) or more advanced techniques like K-Nearest Neighbors (KNN) imputation.\n",
    "\n",
    "### 2. **Data Normalization and Scaling**\n",
    "   - **Min-Max Scaling**: Rescale numerical features to a fixed range, usually [0, 1].\n",
    "   - **Standardization**: Standardize features to have a mean of 0 and a standard deviation of 1.\n",
    "   - **Log Scaling**: Used to reduce the impact of large outliers.\n",
    "\n",
    "### 3. **Categorical Data Encoding**\n",
    "   - **Label Encoding**: Convert categorical labels into numerical values (e.g., 'red' becomes 0, 'blue' becomes 1).\n",
    "   - **One-Hot Encoding**: Convert categories into binary columns, where each column represents a unique category.\n",
    "   - **Ordinal Encoding**: Used when categorical variables have a specific order (e.g., 'low', 'medium', 'high').\n",
    "\n",
    "### 4. **Text Data Processing**\n",
    "   - **Tokenization**: Break text data into individual words or tokens.\n",
    "   - **Lowercasing**: Convert all text to lowercase to reduce dimensionality.\n",
    "   - **Stopword Removal**: Remove common words (e.g., \"the\", \"and\") that don’t contribute much meaning.\n",
    "   - **Stemming/Lemmatization**: Reduce words to their root forms (e.g., \"running\" to \"run\").\n",
    "   - **Text Vectorization**: \n",
    "     - **Bag of Words**: Create a matrix representing word frequency.\n",
    "     - **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weigh word importance based on how often it appears in a document relative to other documents.\n",
    "     - **Word Embeddings**: Use pre-trained embeddings like Word2Vec, GloVe, or train embeddings with models like FastText.\n",
    "\n",
    "### 5. **Handling Outliers**\n",
    "   - **Outlier Detection and Removal**: Identify and remove or cap extreme values using statistical methods (e.g., Z-scores, IQR).\n",
    "\n",
    "### 6. **Feature Selection/Dimensionality Reduction**\n",
    "   - **Filter Methods**: Use correlation metrics to remove redundant features.\n",
    "   - **Wrapper Methods**: Recursive feature elimination (RFE) or stepwise selection to choose relevant features.\n",
    "   - **Dimensionality Reduction**: Apply methods like Principal Component Analysis (PCA) to reduce feature space while preserving variance.\n",
    "\n",
    "### 7. **Data Shuffling**\n",
    "   - Randomize the order of the data to ensure no temporal or systematic order in training.\n",
    "\n",
    "### 8. **Data Splitting**\n",
    "   - Split data into **training**, **validation**, and **test** sets to evaluate model performance correctly.\n",
    "   - Cross-validation can be used for splitting in a more robust manner.\n",
    "\n",
    "### 9. **Feature Engineering**\n",
    "   - **Feature Creation**: Generate new features from existing ones (e.g., combining or transforming variables).\n",
    "   - **Polynomial Features**: Add interactions and powers of existing features.\n",
    "   - **Encoding Time-based Features**: Convert dates into meaningful components (day, month, year, etc.).\n",
    "\n",
    "### 10. **Class Imbalance Handling**\n",
    "   - **Resampling**: Oversample the minority class or undersample the majority class.\n",
    "   - **Synthetic Data Generation**: Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic samples for the minority class.\n",
    "\n",
    "### 11. **Data Augmentation (for Images)**\n",
    "   - Techniques like rotation, flipping, cropping, and zooming are used to artificially expand the training set.\n",
    "\n",
    "### 12. **Feature Scaling for Neural Networks**\n",
    "   - **Normalization/Standardization**: Especially important for networks like deep learning models where features should be on a similar scale.\n",
    "\n",
    "Each preprocessing step helps improve the performance of the model and ensures the data is in a suitable format for feeding into a neural network. The specific steps needed depend on the type of data and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 477, 'name': 'Real Estate Valuation', 'repository_url': 'https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set', 'data_url': 'https://archive.ics.uci.edu/static/public/477/data.csv', 'abstract': 'The real estate valuation is a regression problem. The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. ', 'area': 'Business', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 414, 'num_features': 6, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Y house price of unit area'], 'index_col': ['No'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2018, 'last_updated': 'Mon Feb 26 2024', 'dataset_doi': '10.24432/C5J30W', 'creators': ['I-Cheng Yeh'], 'intro_paper': {'ID': 373, 'type': 'NATIVE', 'title': 'Building real estate valuation models with comparative approach through case-based reasoning', 'authors': 'I. Yeh, Tzu-Kuang Hsu', 'venue': 'Applied Soft Computing', 'year': 2018, 'journal': None, 'DOI': None, 'URL': 'https://dl.acm.org/doi/abs/10.1016/j.asoc.2018.01.029', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. The â€œreal estate valuationâ€\\x9d is a regression problem. The data set was randomly split into the training data set (2/3 samples) and the testing data set (1/3 samples).', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The inputs are as follows\\r\\nX1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)\\r\\nX2=the house age (unit: year)\\r\\nX3=the distance to the nearest MRT station (unit: meter)\\r\\nX4=the number of convenience stores in the living circle on foot (integer)\\r\\nX5=the geographic coordinate, latitude. (unit: degree)\\r\\nX6=the geographic coordinate, longitude. (unit: degree)\\r\\n\\r\\nThe output is as follow\\r\\nY= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)\\r\\n', 'citation': None}}\n",
      "                                     name     role        type demographic  \\\n",
      "0                                      No       ID     Integer        None   \n",
      "1                     X1 transaction date  Feature  Continuous        None   \n",
      "2                            X2 house age  Feature  Continuous        None   \n",
      "3  X3 distance to the nearest MRT station  Feature  Continuous        None   \n",
      "4         X4 number of convenience stores  Feature     Integer        None   \n",
      "5                             X5 latitude  Feature  Continuous        None   \n",
      "6                            X6 longitude  Feature  Continuous        None   \n",
      "7              Y house price of unit area   Target  Continuous        None   \n",
      "\n",
      "                                         description  \\\n",
      "0                                               None   \n",
      "1  for example, 2013.250=2013 March, 2013.500=201...   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4  number of convenience stores in the living cir...   \n",
      "5                    geographic coordinate, latitude   \n",
      "6                   geographic coordinate, longitude   \n",
      "7  10000 New Taiwan Dollar/Ping, where Ping is a ...   \n",
      "\n",
      "                          units missing_values  \n",
      "0                          None             no  \n",
      "1                          None             no  \n",
      "2                          year             no  \n",
      "3                         meter             no  \n",
      "4                       integer             no  \n",
      "5                        degree             no  \n",
      "6                        degree             no  \n",
      "7  10000 New Taiwan Dollar/Ping             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "real_estate_valuation = fetch_ucirepo(id=477) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = real_estate_valuation.data.features \n",
    "y = real_estate_valuation.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(real_estate_valuation.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(real_estate_valuation.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Real Estate Valuation Dataset from an CSV file\n",
    "df = pd.read_csv('REV_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   No  Transaction Date  House Age Distance to MRT Station  \\\n",
      "0   1          2,012.92       32.0                   84.88   \n",
      "1   2          2,012.92       19.5                  306.59   \n",
      "2   3          2,013.58       13.3                  561.98   \n",
      "3   4          2,013.50       13.3                  561.98   \n",
      "4   5          2,012.83        5.0                  390.57   \n",
      "\n",
      "   Number of Convenience Stores  Latitude  Longitude  \\\n",
      "0                            10     24.98     121.54   \n",
      "1                             9     24.98     121.54   \n",
      "2                             5     24.99     121.54   \n",
      "3                             5     24.99     121.54   \n",
      "4                             5     24.98     121.54   \n",
      "\n",
      "   House Price per Unit Area  \n",
      "0                       37.9  \n",
      "1                       42.2  \n",
      "2                       47.3  \n",
      "3                       54.8  \n",
      "4                       43.1  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Identify which preprocessing techniques will be applied to your dataset and explain your reasoning.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **Real Estate Valuation Dataset**, if you had to choose a **single preprocessing technique**, I would recommend focusing on **Feature Scaling** (Normalization or Standardization). \n",
    "\n",
    "### Why Feature Scaling?\n",
    "- **Distance to MRT Station**, **Latitude**, and **Longitude** are numerical features with potentially different ranges, which can lead to biases in models like linear regression, neural networks, and distance-based algorithms (e.g., KNN, SVM).\n",
    "- Without scaling, features with larger ranges can dominate the learning process and negatively impact the model’s performance.\n",
    "- Since the task is likely to involve regression, scaling ensures that the model treats all numerical features on an equal footing.\n",
    "\n",
    "### Recommended Approach:\n",
    "- **Standardization (Z-score scaling)**: This transforms the data so that the features have a mean of 0 and a standard deviation of 1. It works well for many machine learning models, especially for those that assume a normal distribution.\n",
    "\n",
    "- **Min-Max Scaling (Normalization)**: Rescales the features to a range [0, 1], which is useful for algorithms that require inputs to be within a specific range (like neural networks).\n",
    "\n",
    "### Benefits of Feature Scaling for Real Estate Data:\n",
    "- It makes numerical features comparable in scale (e.g., distance, house age, price per unit area).\n",
    "- Prevents large-magnitude features like **distance to MRT** from dominating the learning process.\n",
    "- Many machine learning models (e.g., linear regression, neural networks, k-NN, etc.) perform better and converge faster with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to scale (excluding the target column 'House Price per Unit Area')\n",
    "features_to_scale = ['House Age', 'Distance to MRT Station', 'Number of Convenience Stores', 'Latitude', 'Longitude']\n",
    "target_column = 'House Price per Unit Area'\n",
    "\n",
    "# Clean the features by removing commas and converting to numeric\n",
    "for feature in features_to_scale:\n",
    "    # Remove commas and convert to float\n",
    "    df[feature] = df[feature].replace({',': ''}, regex=True).astype(float)\n",
    "\n",
    "# Clean the target column (if needed)\n",
    "df[target_column] = df[target_column].replace({',': ''}, regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[features_to_scale]\n",
    "y = df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standardization (Z-score scaling)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scaled data back to a DataFrame for easier handling\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=features_to_scale)\n",
    "\n",
    "# Add the target column (House Price per Unit Area) back to the scaled DataFrame\n",
    "df_scaled[target_column] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled Dataset:\n",
      "   House Age  Distance to MRT Station  Number of Convenience Stores  Latitude  \\\n",
      "0   1.255628                -0.792494                      2.007407  0.881540   \n",
      "1   0.157086                -0.616615                      1.667503  0.881540   \n",
      "2  -0.387791                -0.414019                      0.307885  1.659701   \n",
      "3  -0.387791                -0.414019                      0.307885  1.659701   \n",
      "4  -1.117223                -0.549995                      0.307885  0.881540   \n",
      "\n",
      "   Longitude  House Price per Unit Area  \n",
      "0   0.468708                       37.9  \n",
      "1   0.468708                       42.2  \n",
      "2   0.468708                       47.3  \n",
      "3   0.468708                       54.8  \n",
      "4   0.468708                       43.1  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the scaled dataset\n",
    "print(\"\\nScaled Dataset:\")\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled dataset saved as 'REV_Scaled_Dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save the scaled data to a new CSV file (if needed)\n",
    "df_scaled.to_csv('REV_Scaled_Dataset.csv', index=False)\n",
    "print(\"\\nScaled dataset saved as 'REV_Scaled_Dataset.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
